% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv-bss.R
\name{cv.bss}
\alias{cv.bss}
\title{Cross-validated best subset selection}
\usage{
cv.bss(
  x,
  y,
  k = 0:min(nrow(x) - 1, ncol(x), 20),
  mio = "min",
  nfold = 10,
  cv.loss = mspe,
  ...
)
}
\arguments{
\item{x}{a predictor matrix}

\item{y}{a response vector}

\item{k}{the number of predictors to minimise sum of squares over; by default a sequence from 0
to 20}

\item{mio}{one of 'min', 'all', or 'none' indicating whether to run the mixed-integer solver on
the \code{k} that minimises the cv error, all \code{k}, or none at all}

\item{nfold}{the number of folds to use in cross-validation}

\item{cv.loss}{an optional cross-validation loss-function to use; should accept a vector of
errors; by default mean square prediction error}

\item{...}{any other arguments}
}
\value{
See documentation for the \code{cv.rss} function.
}
\description{
Fits a sequence of regression models using best subset selection and then
cross-validates these models. This function is just a wrapper for the \code{cv.rss} function.
The function solves the robust subset selection problem with \code{h}=\code{n}, using nonrobust
measures of location and scale to standardise, and a nonrobust measure of prediction error in
cross-validation.
}
\examples{
# Generate training data
set.seed(123)
n <- 100
p <- 10
p0 <- 5
beta <- c(rep(1, p0), rep(0, p - p0))
x <- matrix(rnorm(n * p), n, p)
e <- rnorm(n)
y <- x \%*\% beta + e

# Best subset selection with cross-validation
fit <- cv.bss(x, y)

# Extract model coefficients, generate predictions, and plot cross-validation results
coef(fit)
predict(fit, x)
plot(fit)
}
\author{
Ryan Thompson <ryan.thompson@monash.edu>
}
